---
title: "Seshat"
author: "Achille-Laurent"
date: "15/04/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Les données
* Seshat global history databank
* Variables complexité sociale etc
* Renormalisée, populations en log
* Mis sous python pour avoir un meileur index
* Inspiré de l'article


## 3. Analyse de la base normalisée
* base date compter comme différente
```{r}
dfn <- read.csv('axial_index.csv',sep=',')
# Mettre l'Index en index et retirer SPC1
data <- subset(dfn, select=-c(Index,Time,NGA,PolID))
rownames(data) <- dfn$Index
attach(data)
head(data)
```
### 3.1. Vue d'ensemble
* boxplot
```{r}
# Séparer les 2 plots qui sont d'échelle différente
boxplot(subset(data,select = c(PolPop,PolTerr,CapPop,levels,money)))
boxplot(subset(data,select = c(government,infrastr,writing,texts)))
```

### 3.2. ACP
* (2D, Prop Variance)
```{r}
X= scale(data, center=T, scale=T) 

S = cov(X)
acp = eigen(S)
lambda = acp$values
vecteurs_propres = acp$vectors
Inertie = sum(diag(S))
part.inertie = lambda/sum(lambda)
barplot(lambda/sum(lambda),names.arg = 1:length(lambda))
title(main="Explication des différentes composantes")

# Les composantes principales : 
C = X %*% vecteurs_propres
# colnames(C) =   paste("comp", 1:4)
plot(C[,1:2],type="p",xlab='PC1',ylab='PC2')
# text(C[,1:2])
title(main="Projection sur les 2 premiers axes principaux")
lines(c(min(C[,1]),max(C[,1])),c(0,0))
lines(c(0,0),c(min(C[,2]),max(C[,2])))

barplot(-vecteurs_propres[,1],ylab = 'Contribution',xlab = 'Variables',names.arg = names(data),axes = TRUE)
title(main="Contributions des variables au PC1")

barplot(-vecteurs_propres[,2],ylab = 'Contribution',xlab = 'Variables',names.arg = names(data),axes = TRUE)
title(main="Contributions des variables au PC2")
```

Remarquer que :
* La premiere composante explique tout : interpréter
* On peut remarquer deux clusters sur l'ACP
* Toutes les variables contribuent de la même façon au PC1, interpréter
* Pour le pc2 on observe des nuances d'ordre similaire, voir cercle
```{r}
library(FactoMineR)
pca <- PCA(data, scale.unit = TRUE, ncp = 11, graph = TRUE)
```

### 3.3 k-means et CAH
On a vu sur l'ACP qu'on pouvait distinguer environ deux groupes. On essaie des k-means :
```{r}
kmeans.result = kmeans(data,3)
plot(C[,1:2],type="p",xlab='PC1',ylab='PC2',col = kmeans.result$cluster+3)
kmeans.result = kmeans(data,2)
plot(C[,1:2],type="p",xlab='PC1',ylab='PC2',col =kmeans.result$cluster)
```


```{r}
kmeans.result$cluster
```

Pour k = 3, on a un groupe qui contient les points dispersés du milieu.
Pour savoir quelle classification est la plus pertinente, essayons un CAH :
```{r}
hc <- hclust(dist(data))
plot(hc,hang=-1,labels = FALSE)
rect.hclust(hc,k=2,border = 4)
rect.hclust(hc,k=3)
barplot(hc$height[(length(hc$height)-10):(length(hc$height))])
```
On ne peut pas vraiment savoir si c'est deux ou 3 classes qu'on peut garder ... Essayons avec pam
```{r}
library(cluster)
pam.result <- pam(data,2)
plot(pam.result)
pam.result <- pam(data,3)
plot(pam.result)
```
Je ne sais pas trop ce que tout cela signifie.
Peut être du côté de la fonction FAMD ?


Une fois les groupes réunis, il serait intéressant de comprendre ce qui les distingue, les deux dimensions n'y suffisent pas entièrement :
```{r}
k = 2
kmeans.result = kmeans(data,k)
plot(C[,1:2],type="p",xlab='PC1',ylab='PC2',col = kmeans.result$cluster)
for(i in 1:k)

data.petit = subset(data,select = c(government,infrastr,writing,texts))
data.grand = subset(data,select = c(PolPop,PolTerr,CapPop,levels,money))
data.grand.all = list()

for(nom in names(data.grand))
{
  for(i in 1:k)
  {
      data.grand.all[[paste(nom,i)]] <- data.grand[kmeans.result$cluster==i,nom]
  }
}
boxplot(data.grand.all,col = rep(c(2:(k+1)),length(data.grand)))

data.petit.all = list()

for(nom in names(data.petit))
{
  for(i in 1:k)
  {
      data.petit.all[[paste(nom,i)]] <- data.petit[kmeans.result$cluster==i,nom]
  }
}
boxplot(data.petit.all,col = rep(c(2:(k+1)),length(data.petit)))

# data.petit.all <- list(data.petit.1$government,data.petit.2$government))
# head(data.petit.all)
```


### 3.4 K-fold
* cross validation

### 3.5 Autres
* tests,lm, beackward forward,
* temps (k means sur évolution PC1)

## 2. Analyse aveec arbres moraux
On utilise les nouveaux groupes ppur tenter de classifier avec des forêts sur les variables morales
On commence par importer la base morale
```{r}
# detach(data)
dfm <- read.csv('morale_index_inaxe.csv',sep=',')
# Mettre l'Index en index et retirer SPC1
datam <- subset(dfm, select=-c(Index,Time,NGA,sum))
rownames(datam) <- dfm$Index
attach(datam)
head(datam)
```

On associe ensuite à chaque individu dans la base morale son cluster (1 ou 2) correspondant
```{r}
rreess <- kmeans.result$cluster
datam$cluster <- (rreess[(dfn$Index)%in%(dfm$Index)] ==1)*1
head(datam)
```
Onpeut ensuite faire un arbre
```{r}
library(rpart)
arbre=rpart(datam$cluster~.,datam)
# summary(arbre)
# print(arbre)
library(rpart.plot)
rpart.plot(arbre, type=4, digits=3)
```
Et une foret
```{r}
# library(randomForest)
# RF <- randomForest(datam$cluster~.,datam)
# pred_RF <- predict(fit_RF,newdata=NewDATA, type="prob")
# pred_RF
```
Ou plutôt de la régression logistique

```{r}
res <- glm(cluster~.,family=binomial,data=datam)
summary(res)
```

```{r}
exp(res$coefficients)
```

```{r}
res0 <- glm(cluster~1,family=binomial,data=datam)
anova(res0,res,test="Chisq")
```
```{r}
library(MASS)
res_AIC <- step(res)  #backward par defaut
```
```{r}
summary(res_AIC)
```

```{r}
anova(res_AIC,res,test="Chisq")
```

```{r}
NewDATA=data.frame(dfm)
pred=predict(res_AIC, newdata=NewDATA, type="response")  #donne proba d'atre dans classe 1
# table(pred,rreess)
```

```{r}
# library(carData)
# vif(res_AIC)
```

```{r}
# par(mfrow=c(2,2))
plot(res_AIC)
```

