---
title: "Seshat"
author: "Achille-Laurent"
date: "03/06/2020"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

# Analyse de la base Seshat
Code concernant la première approche, l'ACP, la classification et la régression

# 1. Données Axial
## 1.1 Importation
```{r}
dfn <- read.csv('../databases/axial_index.csv',sep=',')
# Mettre l'Index en index et retirer SPC1
data <- subset(dfn, select=-c(Index,Time,NGA,PolID))
rownames(data) <- dfn$Index
attach(data)
head(data)
```
## 1.2. Vue d'ensemble
```{r}
# Séparer les 2 plots qui sont d'échelle différente
boxplot(subset(data,select = c(PolPop,PolTerr,CapPop,levels,money)))
boxplot(subset(data,select = c(government,infrastr,writing,texts)))
```

# 2. Analyse en Composante Principale

```{r}
X= scale(data, center=T, scale=T) 

S = cov(X)
acp = eigen(S)
lambda = acp$values
vecteurs_propres = acp$vectors
Inertie = sum(diag(S))
part.inertie = lambda/sum(lambda)
```

## 2.1 Explication
```{r}
barplot(lambda/sum(lambda),names.arg = 1:length(lambda))
title(main="Explication des différentes composantes")
```
## 2.2. Projection
```{r}
# Les composantes principales : 
C = X %*% vecteurs_propres
# colnames(C) =   paste("comp", 1:4)
plot(C[,1:2],type="p",xlab='PC1',ylab='PC2')
# text(C[,1:2])
title(main="Projection sur les 2 premiers axes principaux")
lines(c(min(C[,1]),max(C[,1])),c(0,0))
lines(c(0,0),c(min(C[,2]),max(C[,2])))
```
## 2.3 Composantes principales PC1 et PC2
```{r}
barplot(-vecteurs_propres[,1],ylab = 'Contribution',xlab = 'Variables',names.arg = names(data),axes = TRUE)
title(main="Contributions des variables au PC1")

barplot(-vecteurs_propres[,2],ylab = 'Contribution',xlab = 'Variables',names.arg = names(data),axes = TRUE)
title(main="Contributions des variables au PC2")
```

## 2.4 Avec FactoMineR
```{r}
library(FactoMineR)
pca <- PCA(data, scale.unit = TRUE, ncp = 11, graph = TRUE)
```

# 3 k-means
On a vu sur l'ACP qu'on pouvait distinguer environ deux groupes. On essaie des k-means :

## 3.1 Projection
```{r}
kmeans.result = kmeans(data,3)
plot(C[,1:2],type="p",xlab='PC1',ylab='PC2',col = kmeans.result$cluster+3)
kmeans.result = kmeans(data,2)
plot(C[,1:2],type="p",xlab='PC1',ylab='PC2',col =kmeans.result$cluster)
```

Pour k = 3, on a un groupe qui contient les points dispersés du milieu.
## 3.2 CAH
Pour savoir quelle classification est la plus pertinente, essayons un CAH :
```{r}
hc <- hclust(dist(data))
plot(hc,hang=-1,labels = FALSE)
rect.hclust(hc,k=2,border = 4)
rect.hclust(hc,k=3)
barplot(hc$height[(length(hc$height)-10):(length(hc$height))])
```

## 3.3 Comparaisons des deux clusters
Une fois les groupes réunis, il serait intéressant de comprendre ce qui les distingue, les deux dimensions n'y suffisent pas entièrement :
```{r}
# Kmeans pour k choisi
k = 2 
kmeans.result = kmeans(data,k)
plot(C[,1:2],type="p",xlab='PC1',ylab='PC2',col = kmeans.result$cluster)

# Affichage des grandes variables
data.grand = subset(data,select = c(PolPop,PolTerr,CapPop,levels,money))
data.grand.all = list()
for(nom in names(data.grand))
{
  for(i in 1:k)
  {
      data.grand.all[[paste(nom,i)]] <- data.grand[kmeans.result$cluster==i,nom]
  }
}
boxplot(data.grand.all,col = rep(c(2:(k+1)),length(data.grand)))

# Affichage des petites variables
data.petit = subset(data,select = c(government,infrastr,writing,texts))
data.petit.all = list()
for(nom in names(data.petit))
{
  for(i in 1:k)
  {
      data.petit.all[[paste(nom,i)]] <- data.petit[kmeans.result$cluster==i,nom]
  }
}
boxplot(data.petit.all,col = rep(c(2:(k+1)),length(data.petit)))

```


# 4. Analyse sur la base Morale
On utilise les nouveaux groupes ppur tenter de classifier avec des forêts sur les variables morales
```{r}
detach(data)
```

## 4.1 Création des données 
```{r}
dfm <- read.csv('../databases/morale_index_inaxe.csv',sep=',')

# Mettre l'Index en index et retirer SPC1
datam <- subset(dfm, select=-c(Index,Time,NGA,sum))
rownames(datam) <- dfm$Index
attach(datam)

# On associe ensuite à chaque individu dans la base morale son cluster (1 ou 2) correspondant
rreess <- kmeans.result$cluster
datam$cluster <- (rreess[(dfn$Index)%in%(dfm$Index)] ==1)*1

head(datam)
```

## 4.2 Arbre
```{r}
library(rpart)
arbre=rpart(datam$cluster~.,datam)
# summary(arbre)
# print(arbre)
library(rpart.plot)
rpart.plot(arbre, type=4, digits=3)
```
## 4.3 Régression
## 4.3.1 Modèle
```{r}
res <- glm(cluster~.,family=binomial,data=datam)
summary(res)
```

### 4.3.2 Odds Ratios
```{r}
exp(res$coefficients)
```

### 4.3.3 Pertinence
```{r}
res0 <- glm(cluster~1,family=binomial,data=datam)
anova(res0,res,test="Chisq")
```
### 4.3.4 AIC
```{r}
library(MASS)
res_AIC <- step(res) 
```
```{r}
summary(res_AIC)
```

```{r}
anova(res_AIC,res,test="Chisq")
```

```{r}
par(mfrow=c(2,2))
plot(res_AIC)
```

